# -*- coding: utf-8 -*-
"""TM_Group_12_FINBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fyEC11ARFcoscxVzGQLsoAbVHEGBI5Yz

<div class="alert alert-block alert-info">
<h1> Text Mining Project: Stock Sentiment <br>
Predicting Market Behavior from Tweets</h1><br>
 Text Mining 2025<br>
NOVA IMS MDSAA

# Group 12

|   | Student Name          |  Student ID |
|---|-----------------------|    ---      |
| 1 | Hassan Bhatti       |  20241023 |
| 2 | Moeko Mitani          |   20240670  |
| 3 | Oumayma Ben Hfaiedh   |   20240699  |
| 4 | Rute D'Alva Teixeira      |  20240667  |
| 5 | Sarah Leuthner    |   20240581  |

# Project Overview

The goal of this project is to develop an NLP model capable of predicting Market sentiment based on tweets. <br>
We are going to implement a classification model that receives tweets as inputs and is able to predict, for each tweet, if it describes a **Bearish (0)**, **Bullish (1)**, or **Neutral (2)** attitude.

# Table of Contents

* [<font color='#52b69a'>1 - Data Integration</font>](#1.) <br>
    - [1.1. Import Libraries ](#1.1.)<br>
    - [1.2. Import Data ](#1.2.)<br>  

* [<font color='#52b69a'>2 - Data Preparation</font>](#2.) <br>

* [<font color='#52b69a'>3. FinBERT Model </font>](#3.) <br>
    - [3.1. Initializing the Model ](#3.1.)<br>
    - [3.2. Tokenizer Configuration](#3.2.)<br>  
    - [3.3. Batch Inference Pipeline](#3.3.)<br>
    - [3.4. Predictions](#3.4.)<br>  

* [<font color='#52b69a'>4.  Model Evaluation </font>](#4.) <br>

<a id="1"></a>

# 1. Data Integration

<a class="anchor" id="1.1.">

## 1.1. Import Libraries
<a>
"""

import pandas as pd
import scipy
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
from tqdm import tqdm

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

"""<a class="anchor" id="1.2.">

## 1.2. Import Data
<a>
"""

data = pd.read_csv("train.csv",
                   encoding='unicode_escape',
                   header=0,
                   names=['Text', 'Sentiment'])
data.head()

data.shape

data['Sentiment'].value_counts()

"""# 2. Data Preparation

`Step 1` Ensuring Sentiment is an integer for labelling, as required by the model
"""

# Remove non-numeric Sentiment rows (e.g. header accidentally read as data)
data = data[data['Sentiment'].astype(str).str.isdigit()]

data.dtypes

"""`Step 2` Splitting the dataset, isolating the target from the input features"""

# Split text and labels
X = data['Text'].tolist()
y = data['Sentiment'].tolist()

"""`Step 3` Confirming everything is in place for moddelling:"""

len(y)==len(X)

print(data.head())
print(data.columns)
print(data.shape)
print(data['Sentiment'].isnull().sum())

set(type(label) for label in y)  # Check the types inside the list

"""# 3. FinBERT model

## 3.1. Initializing the model

The pre-trained transformer model and its matching tokenizer are loaded using Hugging Face's AutoModelForSequenceClassification and AutoTokenizer. This loads both the architecture and learned weights optimized for sentiment analysis tasks. The model is set to evaluation mode (model.eval()) to disable dropout layers and ensure consistent inference results.
"""

# 2. Batch prediction with memory management
batch_size = 4  # Conservative for 8GB RAM
preds = []  # Will store your Bullish/Bearish/Neutral labels

tokenizer = AutoTokenizer.from_pretrained("ahmedrachid/FinancialBERT-Sentiment-Analysis")
model = AutoModelForSequenceClassification.from_pretrained("ahmedrachid/FinancialBERT-Sentiment-Analysis")
model.eval()

"""**Testing**

Testing the model performance on some sample tweets present in the dataset, to ensure it recognizes the labels.
"""

test_texts = [
    # Bearish examples
    ('$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean', 0),
    ('$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT', 0),

    # Bullish examples
    ('$BYND - JPMorgan raises price target on Beyond Meat', 1),

    # Neutral examples
    ('$CX - Cemex reports stable quarterly results', 2)
]

print("{:<80} {:<12} {:<12}".format("Text", "Predicted", "Expected"))
print("-" * 100)

for text, true_label in test_texts:
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        logits = model(**inputs).logits
        pred_label = model.config.id2label[logits.argmax().item()]
        confidence = torch.softmax(logits, dim=1).max().item()

    # Convert to your label names
    label_map = {'negative': 'Bearish (0)', 'positive': 'Bullish (1)', 'neutral': 'Neutral (2)'}

    print("{:<80} {:<12} {:<12} (Confidence: {:.2f})".format(
        text[:75] + "..." if len(text) > 75 else text,
        label_map[pred_label],
        ['Bearish (0)', 'Bullish (1)', 'Neutral (2)'][true_label],
        confidence
    ))

"""## 3.2 Tokenizer Configuration

The tokenizer is set up with three key parameters. These settings ensure all input texts are processed into uniformly sized tensors the model can handle.
"""

tokenizer_kwargs = {"return_tensors":"pt", "padding":'longest', "truncation": True, "max_length": 256}

"""## 3.3. Batch Inference Pipeline

In this section, we start by tokenizing input texts, passing them through the model to obtain logits. Then, the pipeline processes texts in batches for efficiency
"""

preds = []
for i in tqdm(range(0, len(X), batch_size)):
    batch = X[i:i+batch_size]
    inputs = tokenizer(batch, **tokenizer_kwargs)

    with torch.no_grad():
        logits = model(**inputs).logits
        batch_preds = [model.config.id2label[p.item()] for p in logits.argmax(dim=1)]
        preds.extend(batch_preds)

    # Verify alignment
    assert len(preds) == i + len(batch), "Prediction count mismatch!"

assert len(preds) == len(X), f"Missing predictions! Expected {len(X)}, got {len(preds)}"

"""## 3.4.  Predictions

The conversion from the previous step creats a score distribution across all possible sentiment classes. The predicted label is selected as the class with the highest probability, representing the model's most confident judgment.
"""

# 3. Verification
print("Sample predictions:", preds[:5])
print("Class distribution:", Counter(preds))

"""### Converting labels to match FINBBERT'S format"""

print(model.config.id2label)

# Ensure y is the same length as X before conversion
assert len(y) == len(X), "Label length mismatch!"

y_true_finbert = [
    'negative' if label == 0 else
    'positive' if label == 1 else
    'neutral'
    for label in y
]

"""# 4. Model Evaluation

Comparing predictions against true labels, using our established evaluation metrics, to validate the modelâ€™s real-world applicability and guide potential improvements.
"""

print(classification_report(
    y_true_finbert,
    preds,
    target_names=['Bearish (negative)', 'Bullish (positive)', 'Neutral'],
    zero_division=0
))

"""### Further analysis"""

print(f"First 3 labels: {y[:3]}")  # Should show ['Bullish', 'Bearish', etc.]
print(f"First 3 preds: {preds[:3]}")  # Should show ['positive', 'negative', etc.]

# Check alignment between true and predicted labels
print("\nSample Alignment:")
for true, pred in zip(y[:5], preds[:5]):
    print(f"True: {true} ({'Bearish' if true==0 else 'Bullish' if true==1 else 'Neutral'}) | Pred: {pred}")

# Verify no data was skipped
assert len(preds) == len(X), f"Missing predictions! Expected {len(X)}, got {len(preds)}"